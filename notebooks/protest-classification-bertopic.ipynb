{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "612a0faa",
   "metadata": {},
   "source": [
    "**Topic Modeling on Protests Dataset using Bertopic**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bcb67f",
   "metadata": {},
   "source": [
    "This notebook uses the Bertopic model for performing topic modeling on protest notes from the ACLED dataset for Iran. The goal is to identify coherent and distinct topics underlying protest narratives and evaluate the model's performance through coherence and topic diversity metrics.\n",
    "We compare three embedding models‚Äî`all-mpnet-base-v2`, `paraphrase-MiniLM-L6-v2`, and `all-MiniLM-L6-v2`‚Äîby computing coherence and diversity, then visualize the `all-mpnet-base-v2` results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaece8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Imports and Environment Setup\n",
    "## Loading all the packages\n",
    "\n",
    "import os  \n",
    "import re  \n",
    "import random \n",
    "from collections import Counter  \n",
    "\n",
    "import numpy as np  \n",
    "import pandas as pd \n",
    "import nltk \n",
    "from nltk.corpus import stopwords  \n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "from bertopic import BERTopic\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.utils import simple_preprocess\n",
    "import matplotlib.pyplot as plt  # plotting library\n",
    "\n",
    "# Prevents parallel tokenizer warnings when using Hugging Face tokenizers\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbbd14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read protest notes from local CSV file\n",
    "data_path = \"/home/ubuntu/Capstone_Files/data/ACELD_Iran.csv\"\n",
    "iran_df = pd.read_csv(data_path, sep=';')\n",
    "\n",
    "## Printing the first five rows of the dataset\n",
    "print(iran_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b224044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_notes(text):\n",
    "    \"\"\"Strip out explicit dates (e.g., '12 March 2020') and\n",
    "    remove words like 'protest', 'rally', etc., to reduce noise.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    # Remove date patterns\n",
    "    text = re.sub(r'\\b\\d{1,2}\\s+\\w+\\s+\\d{4}\\b', '', text)\n",
    "    # Remove protest-related terms\n",
    "    text = re.sub(\n",
    "        r'\\b(protest(?:ed|ing)?|rally|demonstrat(?:ed|ing)?|march|strike|held)\\b',\n",
    "        '',\n",
    "        text,\n",
    "        flags=re.IGNORECASE\n",
    "    )\n",
    "    return text\n",
    "\n",
    "iran_df['clean_notes'] = iran_df['notes'].apply(clean_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ade746c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')  # Downloads the stopwords corpus if not already present\n",
    "\n",
    "# Define custom stopwords to exclude  months and common words\n",
    "custom_stopwords = {\"october\", \"january\", \"february\", \"may\", \"november\", \"december\", \"april\", \"march\", \"front\", \"plan\", \"outside\", \"building\"}\n",
    "\n",
    "# Combine NLTK's English stopwords with the custom list\n",
    "stop_words = set(stopwords.words('english')).union(custom_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c833fb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Tokenize and preprocess input text.\n",
    "    Input:\n",
    "        text (str): raw text string to process\n",
    "    Output:\n",
    "        List[str]: list of cleaned tokens\n",
    "    \"\"\"\n",
    "    # Tokenize text into words, remove non-alphanumeric characters\n",
    "    tokens = simple_preprocess(text, deacc=True)\n",
    "    # Filter tokens: remove stopwords and tokens shorter than three characters\n",
    "    tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing to each note and obtain list of tokens\n",
    "processed_texts = iran_df['clean_notes'].fillna(\"\").apply(preprocess)\n",
    "\n",
    "# Reconstruct documents by joining tokens for BERTopic input\n",
    "docs = processed_texts.apply(lambda tokens: \" \".join(tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08fed68",
   "metadata": {},
   "source": [
    "## UMAP  \n",
    "**Purpose:** Uniform Manifold Approximation and Projection (UMAP) reduces high‚Äëdimensional embeddings to a lower-dimensional space while preserving both local and global structure.  \n",
    "- **`n_neighbors`**: The number of nearest neighbor points used to estimate the manifold structure (balances local vs. global structure).  \n",
    "- **`n_components`**: The dimensionality of the target space (e.g. 5 for 5‚Äëdimensional output).  \n",
    "- **`min_dist`**: The minimum distance between points in the low‚Äëdimensional embedding (controls how tightly UMAP packs points).  \n",
    "- **`metric`**: The distance metric used to compute point‚Äëto‚Äëpoint similarity in the original space (e.g. ‚Äúcosine‚Äù for cosine distance).  \n",
    "- **`random_state`**: Seed for the random number generator to ensure reproducible embeddings.  \n",
    "\n",
    "---\n",
    "\n",
    "## HDBSCAN  \n",
    "**Purpose:** Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN) finds clusters of varying density and marks outliers as noise.  \n",
    "- **`min_cluster_size`**: The smallest size grouping that should be considered a cluster.  \n",
    "- **`min_samples`**: The number of nearby points required to consider a point part of a dense region (higher values ‚Üí more conservative clusters).  \n",
    "- **`metric`**: The distance metric used to measure pairwise distances (e.g. ‚Äúeuclidean‚Äù for straight‚Äëline distance).  \n",
    "- **`prediction_data`**: Whether to store additional data needed for assigning new points to existing clusters.  \n",
    "\n",
    "---\n",
    "\n",
    "## CountVectorizer  \n",
    "**Purpose:** Converts a collection of text documents into a matrix of token (word or n‚Äëgram) counts for feature extraction.  \n",
    "- **`ngram_range=(1,2)`**: Extract both unigrams (single words) and bigrams (two‚Äëword sequences).  \n",
    "- **`stop_words=\"english\"`**: Remove common English stopwords (e.g., ‚Äúthe‚Äù, ‚Äúand‚Äù) before counting.  \n",
    "- **`min_df=5`**: Ignore tokens that appear in fewer than 5 documents to reduce noise and dimensionality.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1388d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_params = {\n",
    "    \"n_neighbors\": 15,\n",
    "    \"n_components\": 5,\n",
    "    \"min_dist\": 0.1,\n",
    "    \"metric\": \"cosine\",\n",
    "    \"random_state\": 42\n",
    "}\n",
    "hdbscan_params = {\n",
    "    \"min_cluster_size\": 20,\n",
    "    \"min_samples\": 10,\n",
    "    \"metric\": \"euclidean\",\n",
    "    \"prediction_data\": True\n",
    "}\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2), stop_words=\"english\", min_df=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e114da19",
   "metadata": {},
   "source": [
    "# SentenceTransformer: \n",
    "A model that converts sentences or documents into fixed-size dense vectors (embeddings) that capture semantic meaning, enabling downstream tasks like clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6aafd8",
   "metadata": {},
   "source": [
    "The **C_V Coherence Score** assesses how semantically coherent the top words within a topic are, based on how often they appear together in the original texts and how similar their meanings are.\n",
    "\n",
    "A higher C_V score indicates that:\n",
    "\n",
    "-- The top words in a topic frequently occur together.\n",
    "\n",
    "-- The words are semantically related.\n",
    "\n",
    "The C_V score combines two key concepts:\n",
    "\n",
    "Co-occurrence Frequency: Measures how often pairs of top words in a topic appear together in a sliding window across the original texts.\n",
    "\n",
    "Semantic Similarity: Evaluates how similar these word pairs are in meaning, typically using cosine similarity over word embeddings.\n",
    "\n",
    "These values are combined using a statistical measure called Normalized Pointwise Mutual Information (NPMI) and aggregated to produce a single coherence score for each topic.\n",
    "\n",
    "\n",
    "### üßÆ C_V Coherence Score Formula\n",
    "\n",
    "The C_V coherence score is calculated as:\n",
    "\n",
    "$$\n",
    "C_V = \\frac{1}{|W|} \\sum_{w_i, w_j \\in W} \\text{NPMI}(w_i, w_j) \\cdot \\text{cosine\\_similarity}(w_i, w_j)\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "$W$ is the set of top words in a topic  \n",
    "$\\text{NPMI}(w_i, w_j)$ is the Normalized Pointwise Mutual Information between words $w_i$ and $w_j$  \n",
    "$\\text{cosine\\_similarity}(w_i, w_j)$ is the semantic similarity between word embeddings of $w_i$ and $w_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455be08c",
   "metadata": {},
   "source": [
    "### Topic Diversity\n",
    "\n",
    "**What it is:**  \n",
    "Topic Diversity measures how distinct the top words are across all topics. A higher score indicates less overlap in the most important words between topics, suggesting more distinctive topic representations.\n",
    "\n",
    "**Formula:**  \n",
    "\n",
    "$\n",
    "\\text{Topic Diversity} = \\frac{\\lvert \\bigcup_{t=1}^{T} W_t \\rvert}{T \\times k}\n",
    "$\n",
    "\n",
    "- $T$ = number of topics (excluding the ‚Äúnoise‚Äù topic)  \n",
    "- $k$ = number of top words considered per topic  \n",
    "- $W_t$ = set of the top $k$ words for topic $t$\n",
    "- $U_{t=1}^{T} W_t$ = union of all these top‚Äëword sets  \n",
    "\n",
    "So you collect the top‚Äëk words from each of the T topics, count how many unique words appear in total, and divide by T x k. A value of 1.0 means no overlap; lower values indicate more shared terms across topics.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af27b231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(embedding_name):\n",
    "    \"\"\"\n",
    "    Fit BERTopic with a given SentenceTransformer embedding model,\n",
    "    compute topic coherence and diversity, and return the scores with the fitted model.\n",
    "    \"\"\"\n",
    "    # Load the specified SentenceTransformer on GPU (if available) to generate embeddings\n",
    "    embed = SentenceTransformer(embedding_name, device='cuda')\n",
    "    \n",
    "    # Instantiate the BERTopic model:\n",
    "    # - embedding_model: our sentence embeddings\n",
    "    # - umap_model: reduces embeddings to lower dimensions\n",
    "    # - hdbscan_model: clusters the reduced embeddings into topics\n",
    "    # - vectorizer_model: extracts n‚Äëgram features for topic representation\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=embed,\n",
    "        umap_model=UMAP(**umap_params),\n",
    "        hdbscan_model=HDBSCAN(**hdbscan_params),\n",
    "        vectorizer_model=vectorizer,\n",
    "        language='english',\n",
    "        calculate_probabilities=True,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Fit BERTopic to our document list, retrieving topic assignments and probabilities\n",
    "    topics, probs = topic_model.fit_transform(docs.tolist())\n",
    "\n",
    "    # Reduce to 30 topics for clearer interpretability\n",
    "    reduced = topic_model.reduce_topics(docs.tolist(), nr_topics=30)\n",
    "    \n",
    "    # Build a list of top words for each topic (excluding the noise topic -1)\n",
    "    topic_words = [\n",
    "        [word for word, _ in topic_model.get_topic(topic_id)]\n",
    "        for topic_id in topic_model.get_topic_info().Topic\n",
    "        if topic_id != -1\n",
    "    ]\n",
    "    \n",
    "    # Prepare a Gensim dictionary from our tokenized texts for coherence calculation\n",
    "    dictionary = Dictionary(processed_texts.tolist())\n",
    "    \n",
    "    # Calculate c_v coherence using the topic word lists and original tokenized texts\n",
    "    cv = CoherenceModel(\n",
    "        topics=topic_words,\n",
    "        texts=processed_texts.tolist(),\n",
    "        dictionary=dictionary,\n",
    "        coherence='c_v'\n",
    "    ).get_coherence()\n",
    "    \n",
    "    # Compute topic diversity:\n",
    "    # - For each topic, collect its top `topk` words\n",
    "    # - Diversity = (unique top words) / (total possible top words)\n",
    "    topk = 10\n",
    "    top_sets = [\n",
    "        set([w for w, _ in topic_model.get_topic(topic_id)[:topk]])\n",
    "        for topic_id in topic_model.get_topic_info().Topic\n",
    "        if topic_id != -1\n",
    "    ]\n",
    "    all_words = [word for s in top_sets for word in s]\n",
    "    diversity = len(set(all_words)) / (topk * len(top_sets))\n",
    "    \n",
    "    # Return the rounded coherence score, diversity score, and the fitted model\n",
    "    return round(cv, 3), round(diversity, 3), topic_model\n",
    "\n",
    "# List of embedding models to compare\n",
    "models = [\n",
    "    \"all-mpnet-base-v2\",\n",
    "    \"paraphrase-MiniLM-L6-v2\",\n",
    "    \"all-MiniLM-L6-v2\"\n",
    "]\n",
    "\n",
    "# Evaluate each model and print results\n",
    "results = {}\n",
    "for name in models:\n",
    "    # Compute coherence, diversity, and retrieve the trained model\n",
    "    cv_score, div_score, mdl = evaluate_model(name)\n",
    "    # Store metrics in a dictionary for later summary\n",
    "    results[name] = {\"Coherence\": cv_score, \"Diversity\": div_score}\n",
    "    # Output the evaluation summary for this embedding\n",
    "    print(f\"{name}: Coherence={cv_score}, Diversity={div_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb8834b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results in a DataFrame for clarity\n",
    "pd.DataFrame(results).T.rename_axis(\"Embedding\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cfe24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpnet_model = results[\"all-mpnet-base-v2\"][\"model\"]  # now defined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5560a6e6",
   "metadata": {},
   "source": [
    "Visualization: Document Counts per Topic \n",
    "This horizontal bar chart shows how many documents each of the 30 topics contains.\n",
    "This chart highlights the relative prevalence of each topic in the corpus, with the top bars representing the most dominant themes and the bottom bars the least common ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ccf5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_info = mpnet_model.get_topic_info()                # full topic info\n",
    "freq = topic_info[topic_info.Topic != -1]                # drop the outlier cluster (-1)\n",
    "freq = freq.sort_values(by=\"Count\", ascending=False)     # sort desc by document count\n",
    "\n",
    "# 2. Plot as horizontal bars\n",
    "plt.figure(figsize=(12, 8))                              # set size to 12√ó8 inches\n",
    "plt.barh(freq['Name'], freq['Count'])                    # barh(names on y, counts on x)\n",
    "plt.xlabel(\"Number of Documents\")                        # label x‚Äëaxis\n",
    "plt.title(\"Document Counts per Topic (all‚Äëmpnet‚Äëbase‚Äëv2)\") # give chart a title\n",
    "plt.gca().invert_yaxis()                                 # invert y so largest bar sits at top\n",
    "plt.tight_layout()                                       # adjust margins\n",
    "plt.show()                                               # render the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843f3ee3",
   "metadata": {},
   "source": [
    "Top 10 Words Before vs. After Preprocessing\n",
    "Two stacked bar charts:\n",
    "- **Top**: raw token frequencies from clean_notes  \n",
    "- **Bottom**: processed token frequencies after stop‚Äëword removal\n",
    "\n",
    "\n",
    "This comparison reveals how preprocessing filters out frequent but uninformative tokens‚Äîprominent in the raw plot‚Äîand surfaces more meaningful, content-specific words in the processed visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e8be79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count raw tokens\n",
    "raw_tokens = []\n",
    "for text in iran_df['clean_notes'].dropna():     # iterate over cleaned notes\n",
    "    raw_tokens.extend(simple_preprocess(text, deacc=True))  # tokenize, strip punctuation\n",
    "raw_counts = Counter(raw_tokens)                          # tally frequencies\n",
    "top10_raw = raw_counts.most_common(10)                    # pick ten most common\n",
    "words_raw, freq_raw = zip(*top10_raw)                     # split into words & counts\n",
    "\n",
    "# Count processed tokens\n",
    "processed_tokens = [w for tokens in processed_texts for w in tokens]  # flatten list\n",
    "proc_counts = Counter(processed_tokens)\n",
    "top10_proc = proc_counts.most_common(10)\n",
    "words_proc, freq_proc = zip(*top10_proc)\n",
    "\n",
    "# Plot both\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))    # two rows, one column\n",
    "\n",
    "# Raw tokens plot\n",
    "ax1.bar(words_raw, freq_raw)                              # bars: x=words_raw, height=freq_raw\n",
    "ax1.set_title(\"Top 10 Words BEFORE Preprocessing\")        # title\n",
    "ax1.set_ylabel(\"Frequency\")                               # y‚Äëaxis label\n",
    "for i, v in enumerate(freq_raw):                          # annotate counts\n",
    "    ax1.text(i, v, str(v), ha='center', va='bottom')\n",
    "\n",
    "# Processed tokens plot\n",
    "ax2.bar(words_proc, freq_proc)\n",
    "ax2.set_title(\"Top 10 Words AFTER Preprocessing\")\n",
    "ax2.set_ylabel(\"Frequency\")\n",
    "for i, v in enumerate(freq_proc):\n",
    "    ax2.text(i, v, str(v), ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()                                        # tidy spacing\n",
    "plt.show()                                                # display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99c8a42",
   "metadata": {},
   "source": [
    "Topic Distribution for 2 Random Documents\n",
    "\n",
    "This chart displays the probability distribution of topics for two individual documents, highlighting which topics each document is most strongly associated with.\n",
    "\n",
    "Purpose: To demonstrate BERTopic‚Äôs soft clustering by showing how a single document can relate to multiple topics with varying strengths, aiding in qualitative inspection of topic assignments. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4193a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select two random doc indices\n",
    "sample_ids = random.sample(range(len(docs)), 2)\n",
    "\n",
    "for doc_id in sample_ids:\n",
    "    # 1. Compute topic probabilities for this document\n",
    "    _, doc_probs = mpnet_model.transform([docs[doc_id]])[0:2]\n",
    "    probs = doc_probs[0]                                   # extract array of probabilities\n",
    "\n",
    "    # 2. Filter for probability > 0.1\n",
    "    significant = [(i, p) for i, p in enumerate(probs) if p > 0.1]\n",
    "    if not significant:\n",
    "        print(f\"Document {doc_id}: No topics >0.1 probability.\\n\")\n",
    "        continue\n",
    "\n",
    "    # 3. Sort descending by probability\n",
    "    significant.sort(key=lambda x: x[1], reverse=True)\n",
    "    topic_labels = [f\"Topic {i}\" for i,_ in significant]   # e.g. \"Topic 5\"\n",
    "    p_vals = [p for _,p in significant]                    # probabilities list\n",
    "\n",
    "    # 4. Print the original text\n",
    "    print(f\"\\nDocument {doc_id}:\\n{iran_df.iloc[doc_id]['notes']}\\n\")\n",
    "\n",
    "    # 5. Plot bar chart\n",
    "    plt.figure(figsize=(10, 4))                            # wide and short figure\n",
    "    bars = plt.bar(topic_labels, p_vals)                   # bars: x=topic_labels, height=p_vals\n",
    "    plt.ylim(0,1)                                          # y‚Äëaxis from 0 to 1\n",
    "    plt.xlabel(\"Topics\")                                   # x‚Äëaxis label\n",
    "    plt.ylabel(\"Probability\")                              # y‚Äëaxis label\n",
    "    plt.title(f\"Topic Distribution for Document {doc_id}\") # chart title\n",
    "    plt.xticks(rotation=45, ha='right')                    # rotate xticks for readability\n",
    "\n",
    "    # 6. Annotate each bar with its probability\n",
    "    for bar, prob in zip(bars, p_vals):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2,\n",
    "                 prob,\n",
    "                 f\"{prob:.2f}\",\n",
    "                 ha='center',\n",
    "                 va='bottom',\n",
    "                 fontsize=9)\n",
    "    plt.tight_layout()                                     # adjust layout\n",
    "    plt.show()                                             # render"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81a82e7",
   "metadata": {},
   "source": [
    "The interactive BERTopic visualization creates an ‚Äúinter‚Äëtopic distance map‚Äù where each topic is shown as a bubble (positioned by its semantic similarity to other topics and sized by its prevalence), and lets you:\n",
    "\n",
    "Hover over a topic to see its top words and their importances\n",
    "\n",
    "Zoom and pan to explore how topics cluster or separate\n",
    "\n",
    "Click into individual topics to inspect term rankings and document examples\n",
    "\n",
    "Its purpose is to give you an intuitive, dynamic way to explore the model‚Äôs topics‚Äîunderstanding which themes are most common, how they relate to one another, and what key words define each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b7a901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the interactive BERTopic visualization object\n",
    "topic_vis_data = mpnet_model.visualize_topics()\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "display(topic_vis_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Capstone_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
